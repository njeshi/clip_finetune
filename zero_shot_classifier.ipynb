{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b3c1a9-d8f2-4c46-b0c7-45d80933514f",
   "metadata": {},
   "source": [
    "### CLIP as a zero shot classifier\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2203844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a54d24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: Tesla K80\n",
      "Torch version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e9ca7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32', 'ViT-B/16']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad338e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"RN101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2f8c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 119,688,033\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f19fbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 classes, 1 templates\n"
     ]
    }
   ],
   "source": [
    "dataset_root = \"data/food-101/images\"\n",
    "\n",
    "class_names = sorted(os.listdir(dataset_root))\n",
    "class_names = [name.replace('_', ' ') for name in class_names]\n",
    "class_to_idx = {class_names[i]: i for i in range(len(class_names))}\n",
    "\n",
    "templates = ['a photo of {}, a type of food.']\n",
    "# class_captions = [f\"a photo of {x}, a type of food.\" for x in class_names]\n",
    "\n",
    "print(f\"{len(class_names)} classes, {len(templates)} templates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16f4df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torchvision.datasets.ImageFolder(root=dataset_root, transform=preprocess)\n",
    "loader = torch.utils.data.DataLoader(images, batch_size=32, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c148d926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 101/101 [00:06<00:00, 15.85it/s]\n"
     ]
    }
   ],
   "source": [
    "def zeroshot_classifier(classnames, templates):\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        for classname in tqdm(classnames):\n",
    "            texts = [template.format(classname) for template in templates] #format with class\n",
    "            texts = clip.tokenize(texts).cuda() #tokenize\n",
    "            class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding)\n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
    "    return zeroshot_weights\n",
    "\n",
    "zeroshot_weights = zeroshot_classifier(class_names, templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3358a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce472611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3157/3157 [49:47<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 80.61\n",
      "Top-5 accuracy: 95.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(loader)):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "\n",
    "top1 = (top1 / n) * 100\n",
    "top5 = (top5 / n) * 100 \n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59b10c-9854-4716-ac0d-d8e2389655ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
