{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b48a63-fd8d-4c24-860c-f272e038ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer\n",
    "\n",
    "from src import FoodDataModule, KPerClassSampler\n",
    "from src import CLIP_Contrastive\n",
    "from src import TextTransformer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa84db51-6c48-4736-903f-b92564f16339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "clip_backbone = \"ViT-B/16\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, image_transform = clip.load(clip_backbone, jit=False)\n",
    "model = model.to(dtype=torch.float32)\n",
    "\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d190457-63fa-4542-af2e-c3e3ec43d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and loaders\n",
    "dataset_root = \"data/food-101/images\"\n",
    "datamodule = FoodDataModule(folder=dataset_root, \n",
    "                            batch_size=32, \n",
    "                            image_transform=image_transform)\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af701b9f-29c8-450e-9477-dd60f36ca24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 1, 77])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Templates and tokenization\n",
    "templates = [\n",
    "    \"a photo of {}, a type of food.\",\n",
    "]\n",
    "\n",
    "text_transformer = TextTransformer(\n",
    "    tokenizer = SimpleTokenizer(), \n",
    "    templates = templates,\n",
    "    context_length = context_length\n",
    ")\n",
    "\n",
    "num_classes = len(datamodule.dataset.class_to_idx.keys())\n",
    "num_captions = len(templates)\n",
    "tokenized_captions = torch.zeros(\n",
    "    (num_classes, num_captions, context_length),\n",
    "    dtype=torch.int)\n",
    "\n",
    "for idx, class_name in datamodule.dataset.idx_to_class.items():\n",
    "    class_captions = text_transformer(class_name)\n",
    "    tokenized_captions[idx] = class_captions\n",
    "    \n",
    "tokenized_captions = tokenized_captions.to(device)\n",
    "tokenized_captions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9781dd-009b-4e43-8ba6-e5fab7180920",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "\n",
    "for k in ks:\n",
    "    print(f'\\nK: {k}')\n",
    "    train_sampler = KPerClassSampler(dataset=datamodule.train_dataset, k=k, seed=42)\n",
    "    \n",
    "    train_loader = datamodule.train_dataloader(train_sampler)\n",
    "    val_loader = datamodule.val_dataloader()\n",
    "    \n",
    "    clip_model = CLIP_Contrastive(model.to(device), tokenized_captions, out_features=512)\n",
    "\n",
    "    log_dir = f'logs/CLIP_contrastive_few_shot_{clip_backbone}_{k}'\n",
    "    logger = TensorBoardLogger(log_dir)\n",
    "    checkpoint = ModelCheckpoint(log_dir, monitor='val/accuracy', mode='max')\n",
    "    epochs = 5\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=1,\n",
    "        max_epochs=epochs,\n",
    "        gradient_clip_val=1,\n",
    "        amp_backend='native',\n",
    "        auto_lr_find=True,\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint]\n",
    "    )\n",
    "\n",
    "    trainer.tune(clip_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "    trainer.fit(clip_model, train_loader, val_loader)\n",
    "    clip_model.load_state_dict(torch.load(checkpoint.best_model_path)['state_dict'])\n",
    "    accuracy = trainer.test(clip_model, datamodule=datamodule)\n",
    "    \n",
    "    k_results[k] = accuracy[0]['test/accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd99d89a-0393-4797-9525-5d5767bfca9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.6 GB\n",
      "Cached:    0.6 GB\n"
     ]
    }
   ],
   "source": [
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bad30b-96c6-4c86-b5f7-2bee4b1eb13a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
