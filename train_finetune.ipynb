{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optimum-router",
   "metadata": {},
   "source": [
    "#### Import required libraries\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "characteristic-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from clip import SimpleTokenizer\n",
    "\n",
    "from src import FoodDataModule, KPerClassSampler\n",
    "from src import CLIP_Captions, CLIP_Linear\n",
    "from src import TextTransformer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-scotland",
   "metadata": {},
   "source": [
    "#### Load the backbone model\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-cartoon",
   "metadata": {},
   "source": [
    "Print all available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acknowledged-association",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32', 'ViT-B/16']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "clip_backbone = \"ViT-B/32\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert device=='cuda', \"No GPU detected in your machine\"\n",
    "\n",
    "model, preprocess = clip.load(clip_backbone, device=device, jit=False)\n",
    "input_resolution = 224\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-cinema",
   "metadata": {},
   "source": [
    "#### Prepare dataset\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "corrected-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"data/food-101/images\"\n",
    "dataset_root = '/Users/fabio/Desktop/dl4cv/data.nosync/'\n",
    "dm = FoodDataModule(folder=dataset_root, batch_size=32, image_transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "blessed-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all caption templates\n",
    "templates = [\n",
    "    \"a photo of {}, a type of food.\"\n",
    "    ]\n",
    "\n",
    "text_transformer = TextTransformer(\n",
    "    tokenizer = SimpleTokenizer(), \n",
    "    templates=templates,\n",
    "    context_length=context_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "approximate-toronto",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 77])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Init tensor of captions\n",
    "num_classes = len(dm.dataset.class_to_idx)\n",
    "num_captions = len(templates)\n",
    "\n",
    "tokenized_captions = torch.zeros((num_classes, num_captions, context_length), dtype=torch.int).to(device)\n",
    "\n",
    "for idx, class_name in dm.dataset.idx_to_class.items():\n",
    "    class_captions = text_transformer(class_name)\n",
    "    tokenized_captions[idx] = class_captions\n",
    "\n",
    "tokenized_captions = tokenized_captions.to(device)\n",
    "tokenized_captions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-george",
   "metadata": {},
   "source": [
    "#### Training\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-centre",
   "metadata": {},
   "source": [
    "`clip_type` refers to the finetuning mode. If type 'captions' then both text and image features are used to train the model else if 'linear' CLIP will be used as an ordinary image classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_type = 'captions'\n",
    "\n",
    "if clip_type == 'captions':\n",
    "    clip_model = CLIP_Captions(model.to(device), tokenized_captions, out_features=512)\n",
    "else:\n",
    "    clip_model = CLIP_Linear(model.to(device), num_classes, out_features=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-prime",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f'logs/CLIP_{clip_type}_{clip_backbone}'\n",
    "logger = TensorBoardLogger(log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir, monitor='val/accuracy', mode='max')\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,\n",
    "                     gradient_clip_val=1,\n",
    "                     auto_lr_find = True,\n",
    "                     logger=logger,\n",
    "                     callbacks=[checkpoint, EarlyStopping(monitor='val/loss')],\n",
    "                     max_epochs = 10\n",
    "                     )\n",
    "\n",
    "# Tune the hyperams, i.e find the best initial lr\n",
    "trainer.tune(clip_wrapper, datamodule = dm)\n",
    "\n",
    "# Start trainig\n",
    "trainer.fit(clip_wrapper, datamodule = dm)\n",
    "\n",
    "# Load best saved model\n",
    "clip_wrapper.load_state_dict(torch.load(checkpoint.best_model_path)['state_dict'])\n",
    "\n",
    "# Test\n",
    "trainer.test(clip_wrapper, datamodule = dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-tension",
   "metadata": {},
   "source": [
    "Load tensorboard and observe performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
